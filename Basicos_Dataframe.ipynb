{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UKzSYhru4MngICK0TX_lZHbGfeHE5aFC",
      "authorship_tag": "ABX9TyOWAluvu2wbcRZtQICDFtA+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luismiguelmartinluengo/PySpark_Demos/blob/main/Basicos_Dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQU2JXJkIP3m",
        "outputId": "c085a50c-b16a-435b-eb0b-7b412cb9f798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import lower, upper"
      ],
      "metadata": {
        "id": "W3tWyi-LIkPN"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''para trabajar con RDD se usa SparkContext. Sirve para trabajar los datos a bajo nivel pero tiene menor funcionalidad y menos integración con otros servicios que SparkSession.\n",
        "  -para trabajar con dataframes hay trabajar con SparkSession, que da acceso a las funcionalidades de de SQL, Dataframe, etc. SparkSession está pensado para trabajar con los\n",
        "  datos a alto nivel\n",
        "'''\n",
        "sparkSession = SparkSession.builder.appName('Basicos Datafrae').getOrCreate()"
      ],
      "metadata": {
        "id": "VNUvR5HdJLc_"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear Dataframe a partir de un RDD\n",
        "datos = [('Hola',1,True,0.1),('como',2,False,0.2),('estan',3,True,0.3),('ustedes',4,False,0.4)]\n",
        "rddDatos = sparkSession.sparkContext.parallelize(datos)\n",
        "print(rddDatos.take(5))\n",
        "#hay que crear el squema correspondiente a los datos del RDD que se pasarán para crear el Dataframe\n",
        "esquemaDatos = StructType([StructField('Palabra', StringType(), True),\n",
        "                           StructField('Orden', IntegerType(), False),\n",
        "                           StructField('Flag', BooleanType(), False),\n",
        "                           StructField('Porcentaje', FloatType(), True)])\n",
        "df = sparkSession.createDataFrame(rddDatos, schema = esquemaDatos)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3NZ9bVAJziF",
        "outputId": "e37924da-8c5e-4d30-cab3-6944aaf98ad7"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hola', 1, True, 0.1), ('como', 2, False, 0.2), ('estan', 3, True, 0.3), ('ustedes', 4, False, 0.4)]\n",
            "+-------+-----+-----+----------+\n",
            "|Palabra|Orden| Flag|Porcentaje|\n",
            "+-------+-----+-----+----------+\n",
            "|   Hola|    1| true|       0.1|\n",
            "|   como|    2|false|       0.2|\n",
            "|  estan|    3| true|       0.3|\n",
            "|ustedes|    4|false|       0.4|\n",
            "+-------+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualización\n",
        "df.show(3,truncate=False, vertical=True)"
      ],
      "metadata": {
        "id": "IMpy64bFOVRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d0739b-70ab-4b5c-9952-6f4fe57e7132"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-RECORD 0-----------\n",
            " Palabra    | Hola  \n",
            " Orden      | 1     \n",
            " Flag       | true  \n",
            " Porcentaje | 0.1   \n",
            "-RECORD 1-----------\n",
            " Palabra    | como  \n",
            " Orden      | 2     \n",
            " Flag       | false \n",
            " Porcentaje | 0.2   \n",
            "-RECORD 2-----------\n",
            " Palabra    | estan \n",
            " Orden      | 3     \n",
            " Flag       | true  \n",
            " Porcentaje | 0.3   \n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualización, puede especificarse el número de filas a mostrar, pero para eso mejor usar take (por claridad de código, porque funcionalmente son lo mismo)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvunqSKziYxb",
        "outputId": "a9f09ad5-7712-4434-ac7f-4d9787eec2aa"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(Palabra='Hola', Orden=1, Flag=True, Porcentaje=0.10000000149011612)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsZkbd5cuTGw",
        "outputId": "b6381695-9578-425c-aaa8-4ad90f967fb3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Palabra='Hola', Orden=1, Flag=True, Porcentaje=0.10000000149011612),\n",
              " Row(Palabra='como', Orden=2, Flag=False, Porcentaje=0.20000000298023224),\n",
              " Row(Palabra='estan', Orden=3, Flag=True, Porcentaje=0.30000001192092896)]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select --> devuelve otro data frame\n",
        "dfSelect = df.select('Palabra', (df.Porcentaje * 100).alias('%'))\n",
        "dfSelect.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847aX1ueixa0",
        "outputId": "4d1959ed-cbf6-4944-ad77-3b1a26d5a485"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|Palabra|        %|\n",
            "+-------+---------+\n",
            "|   Hola|     10.0|\n",
            "|   como|     20.0|\n",
            "|  estan|30.000002|\n",
            "|ustedes|     40.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter (df.where es equivalente a df.filter)\n",
        "dfFilter1 = df.filter((df.Palabra.isin('Hola','como','quien')) | (df.Porcentaje > 0.35)) #como en Pandas, las claúsulas or tienen que estar encapsuladas por paréntesis\n",
        "dfFilter1.show()"
      ],
      "metadata": {
        "id": "z_zwOZe6jsoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61762d64-b55b-4ad9-8ff2-84c2ef5c20b1"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+----------+\n",
            "|Palabra|Orden| Flag|Porcentaje|\n",
            "+-------+-----+-----+----------+\n",
            "|   Hola|    1| true|       0.1|\n",
            "|   como|    2|false|       0.2|\n",
            "|ustedes|    4|false|       0.4|\n",
            "+-------+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#withColumn\n",
        "dfWithColumn = df.withColumn('Palabra Mayuscula', upper(df['Palabra'])).withColumn('Palabra', lower(df['Palabra']))\n",
        "dfWithColumn.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCOQkxzWp1oH",
        "outputId": "51ff4d97-c1fe-4414-84c0-08122910ebd2"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+----------+-----------------+\n",
            "|Palabra|Orden| Flag|Porcentaje|Palabra Mayuscula|\n",
            "+-------+-----+-----+----------+-----------------+\n",
            "|   hola|    1| true|       0.1|             HOLA|\n",
            "|   como|    2|false|       0.2|             COMO|\n",
            "|  estan|    3| true|       0.3|            ESTAN|\n",
            "|ustedes|    4|false|       0.4|          USTEDES|\n",
            "+-------+-----+-----+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pathSuperStore = \"/content/drive/MyDrive/Colab Notebooks/data/Superstore.csv\"\n",
        "dfSuperStore = sparkSession.read.csv(pathSuperStore, header=True, quote = '\"', escape = '\"', inferSchema=True)\n",
        "dfSuperStore.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgWxrIALORq7",
        "outputId": "de67edd0-5dfc-436c-eb24-3dde8501387d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
            "|Row ID|      Order ID|Order Date| Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|   Sales|Quantity|Discount|  Profit|\n",
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
            "|     1|CA-2016-152156| 11/8/2016|11/11/2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|  261.96|       2|     0.0| 41.9136|\n",
            "|     2|CA-2016-152156| 11/8/2016|11/11/2016|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|      42420| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...|  731.94|       3|     0.0| 219.582|\n",
            "|     3|CA-2016-138688| 6/12/2016| 6/16/2016|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|      90036|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|   14.62|       2|     0.0|  6.8714|\n",
            "|     4|US-2015-108966|10/11/2015|10/18/2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|957.5775|       5|    0.45|-383.031|\n",
            "|     5|US-2015-108966|10/11/2015|10/18/2015|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|      33311| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|  22.368|       2|     0.2|  2.5164|\n",
            "+------+--------------+----------+----------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+--------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupBy\n",
        "dfSalesByCity = dfSuperStore.groupBy('City')\\\n",
        "                            .agg({'Sales':'sum'})\\\n",
        "                            .withColumnRenamed('sum(Sales)','Total Sales')\\\n",
        "                            .orderBy('Total Sales', ascending=False)\n",
        "dfSalesByCity.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djrkHV_MO9uW",
        "outputId": "c691e756-d7c7-4e0b-b9ae-5a3421918bde"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------------+\n",
            "|         City|       Total Sales|\n",
            "+-------------+------------------+\n",
            "|New York City|        256368.161|\n",
            "|  Los Angeles|        175851.341|\n",
            "|      Seattle|        119540.742|\n",
            "|San Francisco|112669.09199999992|\n",
            "| Philadelphia|109077.01300000008|\n",
            "+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Window\n",
        "#Reto: identificar el mejor cliente (Customer Name) por beneficio (Profit) de cada Segmento (Segment)\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "dfProfitByCustomer = dfSuperStore.groupBy('Customer Name','Segment')\\\n",
        "                                .agg({'Profit':'sum'})\\\n",
        "                                .withColumnRenamed('sum(Profit)','Total Profit')\n",
        "ventana = Window.partitionBy('Segment').orderBy(dfProfitByCustomer['Total Profit'].desc())\n",
        "dfProfitByCustomer = dfProfitByCustomer.withColumn('rank', row_number().over(ventana))\n",
        "dfTopCustomerBySegment = dfProfitByCustomer.filter(dfProfitByCustomer.rank == 1).drop('rank')\n",
        "dfTopCustomerBySegment.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgnNKTkUQB_P",
        "outputId": "86ce09ba-3db6-4531-a40d-008ed630a6d7"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+-----------------+\n",
            "|Customer Name|    Segment|     Total Profit|\n",
            "+-------------+-----------+-----------------+\n",
            "| Raymond Buch|   Consumer|        6976.0959|\n",
            "| Tamara Chand|  Corporate|8981.323900000001|\n",
            "| Tom Ashbrook|Home Office|4703.788299999999|\n",
            "+-------------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, split\n",
        "dfSalesByYear = dfSuperStore.select('Order Date','Segment','Sales')\n",
        "dfSalesByYear = dfSalesByYear.withColumn('OrderYear', split(dfSalesByYear['Order Date'],'/').getItem(2))\\\n",
        "                            .groupBy('OrderYear','Segment')\\\n",
        "                            .agg({'Sales':'sum'})\\\n",
        "                            .withColumnRenamed('sum(Sales)','Total Sales')\\\n",
        "                            .orderBy(['Segment','OrderYear'], ascending=True)\n",
        "dfSalesByYear.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ7u4_j2T-hA",
        "outputId": "834dd2d3-d4d2-4db7-8d20-26ce58e1e08c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+------------------+\n",
            "|OrderYear|    Segment|       Total Sales|\n",
            "+---------+-----------+------------------+\n",
            "|     2014|   Consumer| 266096.8126000003|\n",
            "|     2015|   Consumer| 266535.9332999999|\n",
            "|     2016|   Consumer|296863.89920000033|\n",
            "|     2017|   Consumer| 331904.6999000011|\n",
            "|     2014|  Corporate|128434.87370000005|\n",
            "|     2015|  Corporate|128757.30689999994|\n",
            "|     2016|  Corporate|207106.36179999987|\n",
            "|     2017|  Corporate|241847.82440000013|\n",
            "|     2014|Home Office| 89715.81180000005|\n",
            "|     2015|Home Office|        75239.2688|\n",
            "|     2016|Home Office|105235.33699999996|\n",
            "|     2017|Home Office|159462.73090000002|\n",
            "+---------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupby + pivot (partiendo del df original)\n",
        "dfSalesPivotYear = dfSuperStore.withColumn('OrderYear', split(dfSuperStore['Order Date'],'/').getItem(2))\\\n",
        "                            .withColumn('Sales', dfSuperStore['Sales'].cast(DoubleType()))\\\n",
        "                            .groupBy('Segment')\\\n",
        "                            .pivot('OrderYear')\\\n",
        "                            .sum('Sales')\\\n",
        "                            .orderBy('Segment')\n",
        "dfSalesPivotYear.show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWGxdOYkg1x_",
        "outputId": "aa60ca46-b4ac-41d4-99bb-2730b7f81b7b"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+------------------+------------------+------------------+\n",
            "|    Segment|              2014|              2015|              2016|              2017|\n",
            "+-----------+------------------+------------------+------------------+------------------+\n",
            "|   Consumer| 266096.8126000003| 266535.9332999999|296863.89920000033| 331904.6999000011|\n",
            "|  Corporate|128434.87370000005|128757.30689999994|207106.36179999987|241847.82440000013|\n",
            "|Home Office| 89715.81180000005|        75239.2688|105235.33699999996|159462.73090000002|\n",
            "+-----------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cálculos múltiples\n",
        "from pyspark.sql.functions import avg, max, min, first\n",
        "dfAgg = dfSuperStore.groupBy('Segment','Region')\\\n",
        "                    .agg(avg('Profit').alias('AvgProfit'),\n",
        "                         max('Quantity').alias('MaxQuantity'),\n",
        "                         min('Quantity').alias('MinQuantity'),\n",
        "                         first('Category').alias('FirstCategoy'))\n",
        "dfAgg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ymHxBP0kFDL",
        "outputId": "43931fc8-30ca-4f74-bb2d-be9d5ed02079"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+------------------+-----------+-----------+---------------+\n",
            "|    Segment| Region|         AvgProfit|MaxQuantity|MinQuantity|   FirstCategoy|\n",
            "+-----------+-------+------------------+-----------+-----------+---------------+\n",
            "|   Consumer|Central| 7.066046287128713|         14|          1|Office Supplies|\n",
            "|   Consumer|   East|28.040152688904023|         14|          1|      Furniture|\n",
            "|   Consumer|  South| 32.11643532219569|         14|          1|      Furniture|\n",
            "|   Consumer|   West| 34.36040909090916|         14|          1|      Furniture|\n",
            "|  Corporate|Central| 27.79183060921253|         11|          1|Office Supplies|\n",
            "|  Corporate|   East| 26.93566579247437|         14|          1|Office Supplies|\n",
            "|  Corporate|  South| 29.83377098039214|         14|          1|Office Supplies|\n",
            "|  Corporate|   West| 35.87232281250003|         14|          1|Office Supplies|\n",
            "|Home Office|Central|28.398201826484033|         14|          1|Office Supplies|\n",
            "|Home Office|   East| 53.20561115537849|         14|          1|      Furniture|\n",
            "|Home Office|  South|16.987626102941178|         14|          1|     Technology|\n",
            "|Home Office|   West| 28.94993870402801|         14|          1|Office Supplies|\n",
            "+-----------+-------+------------------+-----------+-----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rollup\n",
        "#Calculo del total de ventas por Segmento y Región\n",
        "from pyspark.sql.functions import asc_nulls_last\n",
        "dfSalesBySegmentAndRegion = dfSuperStore.rollup('Segment','Region')\\\n",
        "                                        .agg({'Sales':'sum'})\\\n",
        "                                        .withColumnRenamed('sum(Sales)','Total Sales')\\\n",
        "                                        .orderBy([asc_nulls_last('Segment'),asc_nulls_last('Region')])\n",
        "dfSalesBySegmentAndRegion.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPDK8HdtcyAy",
        "outputId": "7cf967d1-e5a8-455a-a797-ed211a2a96b1"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+------------------+\n",
            "|    Segment| Region|       Total Sales|\n",
            "+-----------+-------+------------------+\n",
            "|   Consumer|Central|252031.43400000007|\n",
            "|   Consumer|   East|350908.16700000066|\n",
            "|   Consumer|  South|195580.97100000017|\n",
            "|   Consumer|   West| 362880.7730000003|\n",
            "|   Consumer|   NULL|1161401.3449999888|\n",
            "|  Corporate|Central|       157995.8128|\n",
            "|  Corporate|   East|200409.34699999995|\n",
            "|  Corporate|  South|121885.93250000005|\n",
            "|  Corporate|   West|225855.27449999977|\n",
            "|  Corporate|   NULL| 706146.3668000001|\n",
            "|Home Office|Central| 91212.64399999994|\n",
            "|Home Office|   East|127463.72599999998|\n",
            "|Home Office|  South| 74255.00150000004|\n",
            "|Home Office|   West| 136721.7769999999|\n",
            "|Home Office|   NULL| 429653.1485000003|\n",
            "|       NULL|   NULL| 2297200.860299955|\n",
            "+-----------+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: igual que la celda anterior pero usando cube en vez rollup\n",
        "\n",
        "# Assuming dfSuperStore is your DataFrame\n",
        "dfSalesBySegmentAndRegion = dfSuperStore.cube('Segment','Region')\\\n",
        "                                        .agg({'Sales':'sum'})\\\n",
        "                                        .withColumnRenamed('sum(Sales)','Total Sales')\\\n",
        "                                        .orderBy([asc_nulls_last('Segment'),asc_nulls_last('Region')])\n",
        "dfSalesBySegmentAndRegion.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZeyIFeXjEF-",
        "outputId": "e440e134-5cc8-4761-9974-4a0a5b121796"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+------------------+\n",
            "|    Segment| Region|       Total Sales|\n",
            "+-----------+-------+------------------+\n",
            "|   Consumer|Central|252031.43400000007|\n",
            "|   Consumer|   East|350908.16700000066|\n",
            "|   Consumer|  South|195580.97100000017|\n",
            "|   Consumer|   West| 362880.7730000003|\n",
            "|   Consumer|   NULL|1161401.3449999888|\n",
            "|  Corporate|Central|       157995.8128|\n",
            "|  Corporate|   East|200409.34699999995|\n",
            "|  Corporate|  South|121885.93250000005|\n",
            "|  Corporate|   West|225855.27449999977|\n",
            "|  Corporate|   NULL| 706146.3668000001|\n",
            "|Home Office|Central| 91212.64399999994|\n",
            "|Home Office|   East|127463.72599999998|\n",
            "|Home Office|  South| 74255.00150000004|\n",
            "|Home Office|   West| 136721.7769999999|\n",
            "|Home Office|   NULL| 429653.1485000003|\n",
            "|       NULL|Central| 501239.8908000005|\n",
            "|       NULL|   East| 678781.2399999979|\n",
            "|       NULL|  South| 391721.9050000003|\n",
            "|       NULL|   West| 725457.8245000006|\n",
            "|       NULL|   NULL| 2297200.860299955|\n",
            "+-----------+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputación de valores nulos con ImputerModel\n",
        "from pyspark.sql.functions import when, avg\n",
        "from pyspark.ml.feature import Imputer\n",
        "dfWithNulls = dfSuperStore.withColumn('Sales', when(dfSuperStore.Sales.between(158, 261), None).otherwise(dfSuperStore.Sales.cast(DoubleType())))\n",
        "imputer = Imputer().setInputCol(\"Sales\").setOutputCol(\"SalesImputed\").setStrategy(\"mode\")\n",
        "imputerModel = imputer.fit(dfWithNulls) #Ojo con la documentación de PySpark, según ella el método fit devuelve un \"Transformer\", pero en realidad es un objeto ImputerModel\n",
        "dfWithNulls = imputerModel.transform(dfWithNulls)\n",
        "print('Nulos en columna dfWithNulls.Sales: ', dfWithNulls.filter(dfWithNulls.Sales.isNull()).count())\n",
        "print('Nulos en columna dfWithNulls.SalesImputed: ', dfWithNulls.filter(dfWithNulls.SalesImputed.isNull()).count())\n",
        "print('dfWithNulls.Sales.avg: ', dfWithNulls.select(avg('Sales')).collect()[0][0])\n",
        "print('dfWithNulls.SalesImputed.avg: ', dfWithNulls.select(avg('SalesImputed')).collect()[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoCSf0TAlW_2",
        "outputId": "6cc3372a-53f0-4aed-8b31-37497091aac7"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nulos en columna dfWithNulls.Sales:  881\n",
            "Nulos en columna dfWithNulls.SalesImputed:  0\n",
            "dfWithNulls.Sales.avg:  232.23014214857454\n",
            "dfWithNulls.SalesImputed.avg:  212.900845047022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Columnas Array & explode\n",
        "from pyspark.sql.functions import col, explode\n",
        "dfArray = sparkSession.createDataFrame([('a',[1,2,3]),\n",
        "                                        ('b',[4,5,6]),\n",
        "                                        ('c',[7,8,9])],\n",
        "                                        ['letra','numeros'])\n",
        "dfArray.select(col('numeros')[1]).show()\n",
        "dfArray.withColumn('numero', explode(col('numeros'))).drop('numeros').show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAMbPJUj4d9M",
        "outputId": "7c2de6a2-3823-4ea6-eb13-a7f0b6c733c8"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|numeros[1]|\n",
            "+----------+\n",
            "|         2|\n",
            "|         5|\n",
            "|         8|\n",
            "+----------+\n",
            "\n",
            "+-----+------+\n",
            "|letra|numero|\n",
            "+-----+------+\n",
            "|    a|     1|\n",
            "|    a|     2|\n",
            "|    a|     3|\n",
            "|    b|     4|\n",
            "|    b|     5|\n",
            "|    b|     6|\n",
            "|    c|     7|\n",
            "|    c|     8|\n",
            "|    c|     9|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#StructType\n",
        "esquema = StructType([StructField('nombre', StringType(), True),\n",
        "                      StructField('edad', IntegerType(), True),\n",
        "                      StructField('direccion', StructType([StructField('calle', StringType(), True),\n",
        "                                                           StructField('numero', IntegerType(), True),\n",
        "                                                           StructField('ciudad', StringType(), True)]), True)])\n",
        "datos = [('Juan',25,('Calle 1',123,'Parla')),\n",
        "         ('Maria',30,('Calle 2',456,'Parla')),\n",
        "         ('Pedro',28,('Calle 3',789,'Getafe'))]\n",
        "dfStruct = sparkSession.createDataFrame(datos, schema = esquema)\n",
        "dfStruct.show(truncate = False)\n",
        "print('Pedro vive en', dfStruct.filter(dfStruct.nombre == 'Pedro').select('direccion.ciudad').collect()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njbMq3vmDmwj",
        "outputId": "d3c6fbe5-820c-4973-e05b-4325500125ac"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+----------------------+\n",
            "|nombre|edad|direccion             |\n",
            "+------+----+----------------------+\n",
            "|Juan  |25  |{Calle 1, 123, Parla} |\n",
            "|Maria |30  |{Calle 2, 456, Parla} |\n",
            "|Pedro |28  |{Calle 3, 789, Getafe}|\n",
            "+------+----+----------------------+\n",
            "\n",
            "Pedro vive en Getafe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MapType\n",
        "esquemaMap = StructType([StructField('nombre', StringType(), True),\n",
        "                        StructField('edad', IntegerType(), True),\n",
        "                        StructField('atributos', MapType(StringType(), StringType()), True)])\n",
        "datosMap = [('Juan',25,{'ojos':'azules','altura':'1.75','peso':'87'}),\n",
        "            ('Maria',30,{'ojos':'marrones','altura':'1.65'}),\n",
        "            ('Pedro',28,{'ojos':'verde','pelo':'rubio','altura':'1.80'})]\n",
        "dfMap = sparkSession.createDataFrame(datosMap, schema = esquemaMap)\n",
        "dfMap.show(truncate = False)\n",
        "print('Pedro mide', dfMap.filter(dfMap.nombre == 'Pedro').select(dfMap.atributos['altura']).collect()[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7iiCStHH3KL",
        "outputId": "1ab0a451-852d-478f-b56d-95475e3b5910"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+----------------------------------------------+\n",
            "|nombre|edad|atributos                                     |\n",
            "+------+----+----------------------------------------------+\n",
            "|Juan  |25  |{ojos -> azules, peso -> 87, altura -> 1.75}  |\n",
            "|Maria |30  |{altura -> 1.65, ojos -> marrones}            |\n",
            "|Pedro |28  |{ojos -> verde, pelo -> rubio, altura -> 1.80}|\n",
            "+------+----+----------------------------------------------+\n",
            "\n",
            "Pedro mide 1.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UDF --> pyspark no tiene .apply() de pandas, así que para usar una función personalizada hay que encapsularla dentro de un objeto udf\n",
        "from pyspark.sql.functions import udf\n",
        "def deleteVocales(s):\n",
        "  vocales = 'aeiouAEIOU'\n",
        "  return ''.join([c for c in s if c not in vocales])\n",
        "#End deleteVocales\n",
        "udfDeleteVocales = udf(deleteVocales, StringType())\n",
        "dfMap = dfMap.withColumn('nombre sin vocales', udfDeleteVocales(dfMap.nombre))\n",
        "dfMap.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0MlOMvbX0u8",
        "outputId": "fa47dfa9-c0fc-44a4-849f-8fe65e309e90"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+--------------------+------------------+\n",
            "|nombre|edad|           atributos|nombre sin vocales|\n",
            "+------+----+--------------------+------------------+\n",
            "|  Juan|  25|{ojos -> azules, ...|                Jn|\n",
            "| Maria|  30|{altura -> 1.65, ...|                Mr|\n",
            "| Pedro|  28|{ojos -> verde, p...|               Pdr|\n",
            "+------+----+--------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DqIyUkIfOUH"
      },
      "execution_count": 150,
      "outputs": []
    }
  ]
}